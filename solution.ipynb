{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  The system as described above has its greatest...   \n",
      "1  The <e1>child</e1> was carefully wrapped and b...   \n",
      "2  The <e1>author</e1> of a keygen uses a <e2>dis...   \n",
      "3  A misty <e1>ridge</e1> uprises from the <e2>su...   \n",
      "4  The <e1>student</e1> <e2>association</e2> is t...   \n",
      "\n",
      "                    entities           relation  \n",
      "0  [configuration, elements]    Component-Whole  \n",
      "1            [child, cradle]              Other  \n",
      "2     [author, disassembler]  Instrument-Agency  \n",
      "3             [ridge, surge]              Other  \n",
      "4     [student, association]  Member-Collection  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Caricare il modello di spaCy per l'analisi sintattica e semantica\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Definire le relazioni esistenti nel dataset\n",
    "RELATIONS = [\n",
    "    \"Cause-Effect\", \"Instrument-Agency\", \"Product-Producer\", \"Content-Container\",\n",
    "    \"Entity-Origin\", \"Entity-Destination\", \"Component-Whole\", \"Member-Collection\",\n",
    "    \"Message-Topic\", \"Other\"\n",
    "]\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for i in range(0, len(lines), 2):  # Ogni esempio è su 2 righe (frase + relazione)\n",
    "        if '\"' in lines[i]:  \n",
    "            sentence = re.findall(r'\"(.*?)\"', lines[i])[0]\n",
    "            \n",
    "            # Estrai la relazione senza la parte con le parentesi\n",
    "            relation = re.sub(r'\\(.*?\\)', '', lines[i + 1]).strip() if i + 1 < len(lines) else \"Other\"\n",
    "            \n",
    "            # Estrai le entità\n",
    "            entities = re.findall(r'<e1>(.*?)</e1>', lines[i]) + re.findall(r'<e2>(.*?)</e2>', lines[i])\n",
    "            data.append({'sentence': sentence, 'entities': entities, 'relation': relation})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Caricare i dati\n",
    "train_df = load_data('data/TRAIN_FILE.TXT')\n",
    "\n",
    "# Mostra le prime righe per verifica\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l'idea che ho avuto è di prendere il verbo e di controllarne il prefisso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentence              entities  \\\n",
      "4554  Kevin Kim is surprised by the arrival of an an...     [archer, century]   \n",
      "322   The <e1>name</e1> of the aquarium was derived ...          [name, vote]   \n",
      "6871  The injured <e1>worker</e1> fell from an eleva...     [worker, surface]   \n",
      "5756  A spaghetti <e1>pie</e1> served at a church di...      [pie, poisoning]   \n",
      "1147  Using a teleport, the <e1>protagonist</e1> tra...  [protagonist, place]   \n",
      "\n",
      "     predicted_relation  \n",
      "4554              Other  \n",
      "322               Other  \n",
      "6871              Other  \n",
      "5756              Other  \n",
      "1147  Instrument-Agency  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Caricare il modello di spaCy per il POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lista di prefissi e le loro corrispondenti relazioni\n",
    "prefix_to_relation = {\n",
    "    \"cau\": \"Cause-Effect\",  # Cause, causare\n",
    "    \"lead\": \"Cause-Effect\",  # Lead\n",
    "    \"resu\": \"Cause-Effect\",  # Result\n",
    "    \"use\": \"Instrument-Agency\",  # Use\n",
    "    \"oper\": \"Instrument-Agency\",  # Operate\n",
    "    \"make\": \"Product-Producer\",  # Make\n",
    "    \"prod\": \"Product-Producer\",  # Produce, Produce\n",
    "    \"manu\": \"Product-Producer\",  # Manufacture\n",
    "    \"crea\": \"Product-Producer\",  # Create\n",
    "    \"cont\": \"Content-Container\",  # Contain\n",
    "    \"insi\": \"Content-Container\",  # Inside\n",
    "    \"orig\": \"Entity-Origin\",  # Originate\n",
    "    \"from\": \"Entity-Origin\",  # From\n",
    "    \"move\": \"Entity-Destination\",  # Move\n",
    "    \"to\": \"Entity-Destination\",  # To\n",
    "    \"part\": \"Component-Whole\",  # Part\n",
    "    \"incl\": \"Component-Whole\",  # Include\n",
    "    \"belon\": \"Member-Collection\",  # Belong\n",
    "    \"membe\": \"Member-Collection\",  # Member of\n",
    "    \"talk\": \"Message-Topic\",  # Talk\n",
    "    \"topi\": \"Message-Topic\",  # Topic\n",
    "}\n",
    "\n",
    "def classify_relation_using_verb_with_prefixes(sentence, entities):\n",
    "    # Controlla se entities contiene esattamente due elementi\n",
    "    if len(entities) != 2:\n",
    "        return \"Other\"  # Se non ci sono esattamente due entità, ritorna \"Other\"\n",
    "\n",
    "    # Analizzare la frase con spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Estrarre e1 ed e2 dalle entità\n",
    "    e1, e2 = entities\n",
    "\n",
    "    # Trova il verbo tra e1 ed e2\n",
    "    verb = None\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb = token.lemma_  # Usare la forma lemmatizzata del verbo\n",
    "            break  # Prendiamo il primo verbo che troviamo tra e1 ed e2\n",
    "\n",
    "    # Se c'è un verbo, controlliamo i prefissi\n",
    "    if verb:\n",
    "        verb_prefix = verb[:4]  # Prendi le prime 4 lettere del verbo\n",
    "        if verb_prefix in prefix_to_relation:\n",
    "            return prefix_to_relation[verb_prefix]  # Restituisci la relazione basata sul prefisso\n",
    "\n",
    "    return \"Other\"  # Se nessun prefisso corrisponde\n",
    "\n",
    "# Applicare la funzione al training set\n",
    "train_df['predicted_relation'] = train_df.apply(lambda row: classify_relation_using_verb_with_prefixes(row['sentence'], row['entities']), axis=1)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(train_df[['sentence', 'entities', 'predicted_relation']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.25%\n",
      "                 Class  Precision    Recall  F1-Score  Support\n",
      "0        Entity-Origin   1.000000  0.027027  0.052632       74\n",
      "1         Cause-Effect   0.500000  0.011364  0.022222       88\n",
      "2   Entity-Destination   1.000000  0.042254  0.081081       71\n",
      "3     Product-Producer   0.250000  0.112903  0.155556       62\n",
      "4                Other   0.177953  0.904000  0.297368      125\n",
      "5      Component-Whole   0.250000  0.012048  0.022989       83\n",
      "6    Member-Collection   0.000000  0.000000  0.000000       53\n",
      "7    Instrument-Agency   0.333333  0.093023  0.145455       43\n",
      "8        Message-Topic   0.000000  0.000000  0.000000       44\n",
      "9                        0.000000  0.000000  0.000000       10\n",
      "10   Content-Container   0.333333  0.069767  0.115385       43\n",
      "Macro F1-Score: 0.08\n",
      "Weighted F1-Score: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mf/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Calcolare l'accuratezza\n",
    "accuracy = accuracy_score(train_df['relation'], train_df['predicted_relation'])\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calcolare Precision, Recall e F1-Score per ciascuna classe\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    train_df['relation'], train_df['predicted_relation'], average=None, labels=train_df['relation'].unique()\n",
    ")\n",
    "\n",
    "# Visualizzare i risultati delle metriche per ogni classe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': train_df['relation'].unique(),\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(metrics_df)\n",
    "\n",
    "# Calcolare la F1-Score macro-media\n",
    "macro_f1 = f1.mean()\n",
    "print(f\"Macro F1-Score: {macro_f1:.2f}\")\n",
    "\n",
    "# Calcolare la F1-Score pesata\n",
    "weighted_f1 = (precision * support).sum() / support.sum()\n",
    "print(f\"Weighted F1-Score: {weighted_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature vector + machine learning solution based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione del feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_length  has_verb_prefix_cause  has_verb_prefix_lead  \\\n",
      "0               16                      0                     0   \n",
      "1               14                      0                     0   \n",
      "2               21                      0                     0   \n",
      "3               16                      0                     0   \n",
      "4               10                      0                     0   \n",
      "\n",
      "   has_verb_prefix_resu  has_verb_prefix_use  has_verb_prefix_oper  \\\n",
      "0                     0                    0                     0   \n",
      "1                     0                    0                     0   \n",
      "2                     0                    0                     0   \n",
      "3                     0                    0                     0   \n",
      "4                     0                    1                     0   \n",
      "\n",
      "   has_verb_prefix_make  has_verb_prefix_prod  has_verb_prefix_manu  \\\n",
      "0                     0                     0                     0   \n",
      "1                     0                     0                     0   \n",
      "2                     0                     0                     0   \n",
      "3                     0                     0                     0   \n",
      "4                     0                     0                     0   \n",
      "\n",
      "   has_verb_prefix_crea  ...  has_verb_prefix_from  has_verb_prefix_move  \\\n",
      "0                     0  ...                     0                     0   \n",
      "1                     0  ...                     0                     0   \n",
      "2                     0  ...                     0                     0   \n",
      "3                     0  ...                     0                     0   \n",
      "4                     0  ...                     0                     0   \n",
      "\n",
      "   has_verb_prefix_to  has_verb_prefix_part  has_verb_prefix_incl  \\\n",
      "0                   0                     0                     0   \n",
      "1                   0                     0                     0   \n",
      "2                   0                     0                     0   \n",
      "3                   0                     0                     0   \n",
      "4                   0                     0                     0   \n",
      "\n",
      "   has_verb_prefix_belon  has_verb_prefix_membe  has_verb_prefix_talk  \\\n",
      "0                      0                      0                     0   \n",
      "1                      0                      0                     0   \n",
      "2                      0                      0                     0   \n",
      "3                      0                      0                     0   \n",
      "4                      0                      0                     0   \n",
      "\n",
      "   has_verb_prefix_topi     actual_relation  \n",
      "0                     0       Entity-Origin  \n",
      "1                     0       Entity-Origin  \n",
      "2                     0       Entity-Origin  \n",
      "3                     0        Cause-Effect  \n",
      "4                     0  Entity-Destination  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "14973\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Caricare il modello di spaCy per il POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lista di prefissi e le loro corrispondenti relazioni\n",
    "prefix_to_relation = {\n",
    "    \"cau\": \"Cause-Effect\",  # Cause, causare\n",
    "    \"lead\": \"Cause-Effect\",  # Lead\n",
    "    \"resu\": \"Cause-Effect\",  # Result\n",
    "    \"use\": \"Instrument-Agency\",  # Use\n",
    "    \"oper\": \"Instrument-Agency\",  # Operate\n",
    "    \"make\": \"Product-Producer\",  # Make\n",
    "    \"prod\": \"Product-Producer\",  # Produce, Produce\n",
    "    \"manu\": \"Product-Producer\",  # Manufacture\n",
    "    \"crea\": \"Product-Producer\",  # Create\n",
    "    \"cont\": \"Content-Container\",  # Contain\n",
    "    \"insi\": \"Content-Container\",  # Inside\n",
    "    \"orig\": \"Entity-Origin\",  # Originate\n",
    "    \"from\": \"Entity-Origin\",  # From\n",
    "    \"move\": \"Entity-Destination\",  # Move\n",
    "    \"to\": \"Entity-Destination\",  # To\n",
    "    \"part\": \"Component-Whole\",  # Part\n",
    "    \"incl\": \"Component-Whole\",  # Include\n",
    "    \"belon\": \"Member-Collection\",  # Belong\n",
    "    \"membe\": \"Member-Collection\",  # Member of\n",
    "    \"talk\": \"Message-Topic\",  # Talk\n",
    "    \"topi\": \"Message-Topic\",  # Topic\n",
    "}\n",
    "\n",
    "# Funzione per estrarre il verbo e creare un feature vector basato sui prefissi\n",
    "def extract_features_based_on_verb_with_prefixes(sentence, entities):\n",
    "    # Controlla se entities contiene esattamente due entità\n",
    "    if len(entities) != 2:\n",
    "        return None  # Se non ci sono esattamente due entità, non possiamo estrarre caratteristiche\n",
    "\n",
    "    # Rimuovere i tag delle entità dalla frase\n",
    "    sentence = re.sub(r'<e1>|</e1>|<e2>|</e2>', '', sentence)\n",
    "\n",
    "    # Analizzare la frase con spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Estrarre e1 ed e2 dalle entità\n",
    "    e1, e2 = entities\n",
    "\n",
    "    # Trova il verbo tra e1 ed e2\n",
    "    verb = None\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb = token.lemma_  # Usare la forma lemmatizzata del verbo\n",
    "            break  # Prendiamo il primo verbo che troviamo tra e1 ed e2\n",
    "\n",
    "    # Se non è stato trovato un verbo tra le entità, ritorna None\n",
    "    if not verb:\n",
    "        return None\n",
    "\n",
    "    # Estrarre il prefisso del verbo (le prime 4 lettere)\n",
    "    verb_prefix = verb[:4]  # Prendi le prime 4 lettere del verbo\n",
    "\n",
    "    # Creazione di un feature vector\n",
    "    feature_vector = {\n",
    "        \"sentence_length\": len(sentence.split()),  # Numero di parole nella frase\n",
    "        \"has_verb_prefix_cause\": 1 if verb_prefix == \"cau\" else 0,\n",
    "        \"has_verb_prefix_lead\": 1 if verb_prefix == \"lead\" else 0,\n",
    "        \"has_verb_prefix_resu\": 1 if verb_prefix == \"resu\" else 0,\n",
    "        \"has_verb_prefix_use\": 1 if verb_prefix == \"use\" else 0,\n",
    "        \"has_verb_prefix_oper\": 1 if verb_prefix == \"oper\" else 0,\n",
    "        \"has_verb_prefix_make\": 1 if verb_prefix == \"make\" else 0,\n",
    "        \"has_verb_prefix_prod\": 1 if verb_prefix == \"prod\" else 0,\n",
    "        \"has_verb_prefix_manu\": 1 if verb_prefix == \"manu\" else 0,\n",
    "        \"has_verb_prefix_crea\": 1 if verb_prefix == \"crea\" else 0,\n",
    "        \"has_verb_prefix_cont\": 1 if verb_prefix == \"cont\" else 0,\n",
    "        \"has_verb_prefix_insi\": 1 if verb_prefix == \"insi\" else 0,\n",
    "        \"has_verb_prefix_orig\": 1 if verb_prefix == \"orig\" else 0,\n",
    "        \"has_verb_prefix_from\": 1 if verb_prefix == \"from\" else 0,\n",
    "        \"has_verb_prefix_move\": 1 if verb_prefix == \"move\" else 0,\n",
    "        \"has_verb_prefix_to\": 1 if verb_prefix == \"to\" else 0,\n",
    "        \"has_verb_prefix_part\": 1 if verb_prefix == \"part\" else 0,\n",
    "        \"has_verb_prefix_incl\": 1 if verb_prefix == \"incl\" else 0,\n",
    "        \"has_verb_prefix_belon\": 1 if verb_prefix == \"belon\" else 0,\n",
    "        \"has_verb_prefix_membe\": 1 if verb_prefix == \"membe\" else 0,\n",
    "        \"has_verb_prefix_talk\": 1 if verb_prefix == \"talk\" else 0,\n",
    "        \"has_verb_prefix_topi\": 1 if verb_prefix == \"topi\" else 0,\n",
    "    }\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "# Applicare la funzione al training set per creare il feature vector\n",
    "train_features = []\n",
    "for _, row in train_df.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    entities = row['entities']\n",
    "    feature_vector = extract_features_based_on_verb_with_prefixes(sentence, entities)\n",
    "    \n",
    "    if feature_vector:  # Aggiungi solo se c'è un feature vector\n",
    "        feature_vector['actual_relation'] = row['relation']\n",
    "        train_features.append(feature_vector)\n",
    "\n",
    "# Creazione del dataframe con i feature vectors\n",
    "train_feature_df = pd.DataFrame(train_features)\n",
    "\n",
    "# Visualizza i primi 5 feature vector\n",
    "print(train_feature_df.head())\n",
    "print(train_feature_df.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1908\n",
      "Confusion Matrix:\n",
      "[[ 0  4  0  0  0  0  0  0 16  0]\n",
      " [ 0  2  0  0  0  1  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  1  0  0  0  0 11  0]\n",
      " [ 0  2  0  0  0  0  0  0  9  2]\n",
      " [ 0  1  0  0  0  3  0  0  4  0]\n",
      " [ 0  1  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0 16  0]\n",
      " [ 0  3  0  1  0  0  0  0 18  1]\n",
      " [ 0  3  0  0  0  0  0  0  7  1]]\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      Cause-Effect       0.00      0.00      0.00        20\n",
      "   Component-Whole       0.12      0.18      0.15        11\n",
      " Content-Container       0.00      0.00      0.00         7\n",
      "Entity-Destination       0.50      0.08      0.14        12\n",
      "     Entity-Origin       0.00      0.00      0.00        13\n",
      " Instrument-Agency       0.75      0.38      0.50         8\n",
      " Member-Collection       0.00      0.00      0.00        10\n",
      "     Message-Topic       0.00      0.00      0.00        16\n",
      "             Other       0.17      0.78      0.28        23\n",
      "  Product-Producer       0.25      0.09      0.13        11\n",
      "\n",
      "          accuracy                           0.19       131\n",
      "         macro avg       0.18      0.15      0.12       131\n",
      "      weighted avg       0.15      0.19      0.12       131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mf/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mf/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mf/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assicurati che la colonna di relazione (target) sia separata\n",
    "X = train_feature_df.drop(columns=['actual_relation'])  # Le caratteristiche\n",
    "y = train_feature_df['actual_relation']  # Le etichette (relazioni effettive)\n",
    "\n",
    "# Suddividere i dati in training e validation set (80% / 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creare il modello SVM\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Allenare il modello\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predire le etichette sul set di validazione\n",
    "y_pred = svm_model.predict(X_val)\n",
    "\n",
    "# Calcolare l'accuratezza\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Matrice di confusione\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Report di classificazione\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39985/1272719405.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_39985/1272719405.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training Loss: 2.2629, Accuracy: 0.1825\n",
      "Validation Loss: 2.0538, Accuracy: 0.2874\n",
      "Test Loss: 2.1287, Accuracy: 0.2299\n",
      "Epoch 2/3\n",
      "Training Loss: 1.8115, Accuracy: 0.3736\n",
      "Validation Loss: 1.5244, Accuracy: 0.5057\n",
      "Test Loss: 1.6548, Accuracy: 0.4828\n",
      "Epoch 3/3\n",
      "Training Loss: 1.2332, Accuracy: 0.6609\n",
      "Validation Loss: 1.1375, Accuracy: 0.6322\n",
      "Test Loss: 1.1666, Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.cuda.amp import autocast, GradScaler  # For mixed precision\n",
    "\n",
    "# Load the DistilBERT tokenizer and model (smaller model)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Label encoding for relations\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['relation_label'] = label_encoder.fit_transform(train_df['relation'])\n",
    "\n",
    "# Split train_df into train (80%), val (10%), and test (10%)\n",
    "train_df, test_val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Custom Dataset for BERT tokenization\n",
    "class RelationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=32):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        sentence = row['sentence']\n",
    "        relation_label = row['relation_label']\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        encoding = self.tokenizer(\n",
    "            sentence, \n",
    "            add_special_tokens=True, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(relation_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Instantiate Datasets and DataLoaders\n",
    "train_dataset = RelationDataset(train_df, tokenizer)\n",
    "val_dataset = RelationDataset(val_df, tokenizer)\n",
    "test_dataset = RelationDataset(test_df, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "class SelfAttentionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_labels):\n",
    "        super(SelfAttentionModel, self).__init__()\n",
    "        \n",
    "        # Caricamento di un modello più piccolo (DistilBERT) per ridurre la memoria\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Layer Fully Connected per la classificazione\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Ottenere l'output di DistilBERT (last hidden state)\n",
    "        distilbert_output = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Estrarre l'output dell'ultimo stato nascosto\n",
    "        hidden_state = distilbert_output.last_hidden_state\n",
    "        \n",
    "        # Usare l'output del token [CLS] per la classificazione (primo token)\n",
    "        cls_token_output = hidden_state[:, 0, :]  # Prendere il primo token (CLS)\n",
    "\n",
    "        # Passare attraverso un layer fully connected per ottenere le predizioni\n",
    "        logits = self.fc(cls_token_output)  # La forma ora è (batch_size, num_labels)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = SelfAttentionModel(hidden_dim=768, num_labels=len(label_encoder.classes_))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():  # Use mixed precision\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass with mixed precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation Loop\n",
    "def eval_epoch(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training and Evaluation\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, loss_fn, optimizer)\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    val_loss, val_accuracy = eval_epoch(model, val_dataloader, loss_fn)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_loss, test_accuracy = eval_epoch(model, test_dataloader, loss_fn)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Clear GPU memory after each epoch\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
