{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  The system as described above has its greatest...   \n",
      "1  The <e1>child</e1> was carefully wrapped and b...   \n",
      "2  The <e1>author</e1> of a keygen uses a <e2>dis...   \n",
      "3  A misty <e1>ridge</e1> uprises from the <e2>su...   \n",
      "4  The <e1>student</e1> <e2>association</e2> is t...   \n",
      "\n",
      "                    entities           relation  \n",
      "0  [configuration, elements]    Component-Whole  \n",
      "1            [child, cradle]              Other  \n",
      "2     [author, disassembler]  Instrument-Agency  \n",
      "3             [ridge, surge]              Other  \n",
      "4     [student, association]  Member-Collection  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Caricare il modello di spaCy per l'analisi sintattica e semantica\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Definire le relazioni esistenti nel dataset\n",
    "RELATIONS = [\n",
    "    \"Cause-Effect\", \"Instrument-Agency\", \"Product-Producer\", \"Content-Container\",\n",
    "    \"Entity-Origin\", \"Entity-Destination\", \"Component-Whole\", \"Member-Collection\",\n",
    "    \"Message-Topic\", \"Other\"\n",
    "]\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for i in range(0, len(lines), 2):  # Ogni esempio è su 2 righe (frase + relazione)\n",
    "        if '\"' in lines[i]:  \n",
    "            sentence = re.findall(r'\"(.*?)\"', lines[i])[0]\n",
    "            \n",
    "            # Estrai la relazione senza la parte con le parentesi\n",
    "            relation = re.sub(r'\\(.*?\\)', '', lines[i + 1]).strip() if i + 1 < len(lines) else \"Other\"\n",
    "            \n",
    "            # Estrai le entità\n",
    "            entities = re.findall(r'<e1>(.*?)</e1>', lines[i]) + re.findall(r'<e2>(.*?)</e2>', lines[i])\n",
    "            data.append({'sentence': sentence, 'entities': entities, 'relation': relation})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Caricare i dati\n",
    "train_df = load_data('data/TRAIN_FILE.TXT')\n",
    "\n",
    "# Mostra le prime righe per verifica\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l'idea che ho avuto è di prendere il verbo e di controllarne il prefisso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  The system as described above has its greatest...   \n",
      "1  The <e1>child</e1> was carefully wrapped and b...   \n",
      "2  The <e1>author</e1> of a keygen uses a <e2>dis...   \n",
      "3  A misty <e1>ridge</e1> uprises from the <e2>su...   \n",
      "4  The <e1>student</e1> <e2>association</e2> is t...   \n",
      "\n",
      "                    entities predicted_relation  \n",
      "0  [configuration, elements]              Other  \n",
      "1            [child, cradle]              Other  \n",
      "2     [author, disassembler]  Instrument-Agency  \n",
      "3             [ridge, surge]              Other  \n",
      "4     [student, association]              Other  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Caricare il modello di spaCy per il POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lista di prefissi e le loro corrispondenti relazioni\n",
    "prefix_to_relation = {\n",
    "    \"cau\": \"Cause-Effect\",  # Cause, causare\n",
    "    \"lead\": \"Cause-Effect\",  # Lead\n",
    "    \"resu\": \"Cause-Effect\",  # Result\n",
    "    \"use\": \"Instrument-Agency\",  # Use\n",
    "    \"oper\": \"Instrument-Agency\",  # Operate\n",
    "    \"make\": \"Product-Producer\",  # Make\n",
    "    \"prod\": \"Product-Producer\",  # Produce, Produce\n",
    "    \"manu\": \"Product-Producer\",  # Manufacture\n",
    "    \"crea\": \"Product-Producer\",  # Create\n",
    "    \"cont\": \"Content-Container\",  # Contain\n",
    "    \"insi\": \"Content-Container\",  # Inside\n",
    "    \"orig\": \"Entity-Origin\",  # Originate\n",
    "    \"from\": \"Entity-Origin\",  # From\n",
    "    \"move\": \"Entity-Destination\",  # Move\n",
    "    \"to\": \"Entity-Destination\",  # To\n",
    "    \"part\": \"Component-Whole\",  # Part\n",
    "    \"incl\": \"Component-Whole\",  # Include\n",
    "    \"belon\": \"Member-Collection\",  # Belong\n",
    "    \"membe\": \"Member-Collection\",  # Member of\n",
    "    \"talk\": \"Message-Topic\",  # Talk\n",
    "    \"topi\": \"Message-Topic\",  # Topic\n",
    "}\n",
    "\n",
    "def classify_relation_using_verb_with_prefixes(sentence, entities):\n",
    "    # Controlla se entities contiene esattamente due elementi\n",
    "    if len(entities) != 2:\n",
    "        return \"Other\"  # Se non ci sono esattamente due entità, ritorna \"Other\"\n",
    "\n",
    "    # Analizzare la frase con spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Estrarre e1 ed e2 dalle entità\n",
    "    e1, e2 = entities\n",
    "\n",
    "    # Trova il verbo tra e1 ed e2\n",
    "    verb = None\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb = token.lemma_  # Usare la forma lemmatizzata del verbo\n",
    "            break  # Prendiamo il primo verbo che troviamo tra e1 ed e2\n",
    "\n",
    "    # Se c'è un verbo, controlliamo i prefissi\n",
    "    if verb:\n",
    "        verb_prefix = verb[:4]  # Prendi le prime 4 lettere del verbo\n",
    "        if verb_prefix in prefix_to_relation:\n",
    "            return prefix_to_relation[verb_prefix]  # Restituisci la relazione basata sul prefisso\n",
    "\n",
    "    return \"Other\"  # Se nessun prefisso corrisponde\n",
    "\n",
    "# Applicare la funzione al training set\n",
    "train_df['predicted_relation'] = train_df.apply(lambda row: classify_relation_using_verb_with_prefixes(row['sentence'], row['entities']), axis=1)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(train_df[['sentence', 'entities', 'predicted_relation']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.39%\n",
      "                 Class  Precision    Recall  F1-Score  Support\n",
      "0      Component-Whole   0.296875  0.020191  0.037811      941\n",
      "1                Other   0.176502  0.929078  0.296649     1410\n",
      "2    Instrument-Agency   0.354839  0.087302  0.140127      504\n",
      "3    Member-Collection   0.000000  0.000000  0.000000      690\n",
      "4         Cause-Effect   0.642857  0.026919  0.051675     1003\n",
      "5   Entity-Destination   0.651163  0.033136  0.063063      845\n",
      "6    Content-Container   0.426357  0.101852  0.164425      540\n",
      "7        Message-Topic   0.300000  0.004732  0.009317      634\n",
      "8     Product-Producer   0.285714  0.108787  0.157576      717\n",
      "9        Entity-Origin   0.909091  0.013966  0.027510      716\n",
      "10                       0.000000  0.000000  0.000000      118\n",
      "Macro F1-Score: 0.09\n",
      "Weighted F1-Score: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mf/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Calcolare l'accuratezza\n",
    "accuracy = accuracy_score(train_df['relation'], train_df['predicted_relation'])\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calcolare Precision, Recall e F1-Score per ciascuna classe\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    train_df['relation'], train_df['predicted_relation'], average=None, labels=train_df['relation'].unique()\n",
    ")\n",
    "\n",
    "# Visualizzare i risultati delle metriche per ogni classe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': train_df['relation'].unique(),\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(metrics_df)\n",
    "\n",
    "# Calcolare la F1-Score macro-media\n",
    "macro_f1 = f1.mean()\n",
    "print(f\"Macro F1-Score: {macro_f1:.2f}\")\n",
    "\n",
    "# Calcolare la F1-Score pesata\n",
    "weighted_f1 = (precision * support).sum() / support.sum()\n",
    "print(f\"Weighted F1-Score: {weighted_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature vector + machine learning solution based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione del feature vector, bool features controllando determinate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_length  e1_is_ADJ  e2_is_ADJ  e1_is_ADP  e2_is_ADP  e1_is_ADV  \\\n",
      "0               27          0          0          0          0          0   \n",
      "1                6          0          0          0          0          0   \n",
      "2               24          0          0          0          0          0   \n",
      "3               14          0          0          0          0          0   \n",
      "4               13          0          0          0          0          0   \n",
      "\n",
      "   e2_is_ADV  e1_is_AUX  e2_is_AUX  e1_is_CCONJ  ...  has_from  has_move  \\\n",
      "0          0          0          0            0  ...         0         0   \n",
      "1          0          0          0            0  ...         0         0   \n",
      "2          0          0          0            0  ...         0         0   \n",
      "3          0          0          0            0  ...         0         0   \n",
      "4          0          0          0            0  ...         0         0   \n",
      "\n",
      "   has_to  has_part  has_include  has_belong  has_member  has_talk  has_topic  \\\n",
      "0       1         0            0           0           0         0          0   \n",
      "1       0         0            0           0           0         0          0   \n",
      "2       1         0            1           0           0         0          0   \n",
      "3       0         0            0           0           0         0          0   \n",
      "4       1         0            0           0           0         0          0   \n",
      "\n",
      "      actual_relation  \n",
      "0    Product-Producer  \n",
      "1  Entity-Destination  \n",
      "2    Product-Producer  \n",
      "3               Other  \n",
      "4   Content-Container  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "365313\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the set of all possible POS tags in spaCy\n",
    "possible_pos_tags = [\n",
    "    \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\",\n",
    "    \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"\n",
    "]\n",
    "\n",
    "# List of individual keywords (lemmatized)\n",
    "keywords = [\n",
    "    \"cause\", \"lead\", \"result\", \"use\", \"operate\", \"make\", \"produce\", \"manufacture\", \"create\",\n",
    "    \"contain\", \"inside\", \"originate\", \"from\", \"move\", \"to\", \"part\", \"include\",\n",
    "    \"belong\", \"member\", \"talk\", \"topic\"\n",
    "]\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(sentence, entities):\n",
    "    # Check if exactly two entities exist\n",
    "    if len(entities) != 2:\n",
    "        return None  \n",
    "\n",
    "    # Remove entity tags from the sentence\n",
    "    sentence_cleaned = re.sub(r'<e1>|</e1>|<e2>|</e2>', '', sentence)\n",
    "\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence_cleaned)\n",
    "\n",
    "    # Extract entity texts\n",
    "    e1, e2 = entities\n",
    "\n",
    "    # Initialize feature vector\n",
    "    feature_vector = {\"sentence_length\": len(sentence_cleaned.split())}\n",
    "\n",
    "    # Initialize one-hot encoding for POS tags (all 0 by default)\n",
    "    for pos in possible_pos_tags:\n",
    "        feature_vector[f\"e1_is_{pos}\"] = 0\n",
    "        feature_vector[f\"e2_is_{pos}\"] = 0\n",
    "\n",
    "    # Find the POS tags for e1 and e2\n",
    "    for token in doc:\n",
    "        if token.text == e1:\n",
    "            if token.pos_ in possible_pos_tags:\n",
    "                feature_vector[f\"e1_is_{token.pos_}\"] = 1\n",
    "        elif token.text == e2:\n",
    "            if token.pos_ in possible_pos_tags:\n",
    "                feature_vector[f\"e2_is_{token.pos_}\"] = 1\n",
    "\n",
    "    # Create binary features for each keyword\n",
    "    lemmas_in_sentence = {token.lemma_ for token in doc}  \n",
    "    for keyword in keywords:\n",
    "        feature_vector[f\"has_{keyword}\"] = 1 if keyword in lemmas_in_sentence else 0\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "# Apply the function to the training set\n",
    "train_features = []\n",
    "for _, row in train_df.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    entities = row['entities']\n",
    "    feature_vector = extract_features(sentence, entities)\n",
    "    \n",
    "    if feature_vector:\n",
    "        feature_vector['actual_relation'] = row['relation']\n",
    "        train_features.append(feature_vector)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "train_feature_df = pd.DataFrame(train_features)\n",
    "\n",
    "# Display the first 5 feature vectors\n",
    "print(train_feature_df.head())\n",
    "print(train_feature_df.size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3300\n",
      "Confusion Matrix:\n",
      "[[ 88   4   0   6  33   0   1   0  17   8]\n",
      " [  4  13   6  13  10   7   2   1  91   0]\n",
      " [  1   1  21  11   3   0   0   0  32   0]\n",
      " [  3   4   3  80   7   3   0   0  41   1]\n",
      " [  2   1   2   6  70   2   0   0  25   1]\n",
      " [  1   2   0  11   6  22   0   0  32   0]\n",
      " [  2   3   2  14  14   4   3   0  85   0]\n",
      " [  2   1   1  32  12   5   0   7  57   1]\n",
      " [  7   6   3  43  40  10   1   0 105   4]\n",
      " [  2   1   0   7  19   5   1   0  71  14]]\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      Cause-Effect       0.79      0.56      0.65       157\n",
      "   Component-Whole       0.36      0.09      0.14       147\n",
      " Content-Container       0.55      0.30      0.39        69\n",
      "Entity-Destination       0.36      0.56      0.44       142\n",
      "     Entity-Origin       0.33      0.64      0.43       109\n",
      " Instrument-Agency       0.38      0.30      0.33        74\n",
      " Member-Collection       0.38      0.02      0.04       127\n",
      "     Message-Topic       0.88      0.06      0.11       118\n",
      "             Other       0.19      0.48      0.27       219\n",
      "  Product-Producer       0.48      0.12      0.19       120\n",
      "\n",
      "          accuracy                           0.33      1282\n",
      "         macro avg       0.47      0.31      0.30      1282\n",
      "      weighted avg       0.45      0.33      0.30      1282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assicurati che la colonna di relazione (target) sia separata\n",
    "X = train_feature_df.drop(columns=['actual_relation'])  # Le caratteristiche\n",
    "y = train_feature_df['actual_relation']  # Le etichette (relazioni effettive)\n",
    "\n",
    "# Suddividere i dati in training e validation set (80% / 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creare il modello SVM\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Allenare il modello\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predire le etichette sul set di validazione\n",
    "y_pred = svm_model.predict(X_val)\n",
    "\n",
    "# Calcolare l'accuratezza\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Matrice di confusione\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Report di classificazione\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training Loss: 1.9720, Accuracy: 0.3238\n",
      "Validation Loss: 1.2996, Accuracy: 0.6325\n",
      "Test Loss: 1.3806, Accuracy: 0.5766\n",
      "Epoch 2/5\n",
      "Training Loss: 1.0070, Accuracy: 0.6739\n",
      "Validation Loss: 0.8197, Accuracy: 0.7229\n",
      "Test Loss: 0.9844, Accuracy: 0.6486\n",
      "Epoch 3/5\n",
      "Training Loss: 0.6856, Accuracy: 0.7841\n",
      "Validation Loss: 0.7382, Accuracy: 0.7530\n",
      "Test Loss: 0.8632, Accuracy: 0.6877\n",
      "Epoch 4/5\n",
      "Training Loss: 0.4984, Accuracy: 0.8469\n",
      "Validation Loss: 0.7046, Accuracy: 0.7470\n",
      "Test Loss: 0.8593, Accuracy: 0.6817\n",
      "Epoch 5/5\n",
      "Training Loss: 0.3573, Accuracy: 0.9003\n",
      "Validation Loss: 0.7446, Accuracy: 0.7319\n",
      "Test Loss: 0.8976, Accuracy: 0.6847\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.amp import autocast, GradScaler  # For mixed precision\n",
    "\n",
    "\n",
    "# Load the DistilBERT tokenizer and model (smaller model)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Label encoding for relations\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['relation_label'] = label_encoder.fit_transform(train_df['relation'])\n",
    "\n",
    "# Split train_df into train (80%), val (10%), and test (10%)\n",
    "train_df, test_val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Custom Dataset for BERT tokenization\n",
    "class RelationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=32):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        sentence = row['sentence']\n",
    "        relation_label = row['relation_label']\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        encoding = self.tokenizer(\n",
    "            sentence, \n",
    "            add_special_tokens=True, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(relation_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Instantiate Datasets and DataLoaders\n",
    "train_dataset = RelationDataset(train_df, tokenizer)\n",
    "val_dataset = RelationDataset(val_df, tokenizer)\n",
    "test_dataset = RelationDataset(test_df, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "class SelfAttentionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_labels):\n",
    "        super(SelfAttentionModel, self).__init__()\n",
    "        \n",
    "        # Caricamento di un modello più piccolo (DistilBERT) per ridurre la memoria\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Layer Fully Connected per la classificazione\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Ottenere l'output di DistilBERT (last hidden state)\n",
    "        distilbert_output = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Estrarre l'output dell'ultimo stato nascosto\n",
    "        hidden_state = distilbert_output.last_hidden_state\n",
    "        \n",
    "        # Usare l'output del token [CLS] per la classificazione (primo token)\n",
    "        cls_token_output = hidden_state[:, 0, :]  # Prendere il primo token (CLS)\n",
    "\n",
    "        # Passare attraverso un layer fully connected per ottenere le predizioni\n",
    "        logits = self.fc(cls_token_output)  # La forma ora è (batch_size, num_labels)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = SelfAttentionModel(hidden_dim=768, num_labels=len(label_encoder.classes_))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=\"cuda\"):  # Use mixed precision\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass with mixed precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation Loop\n",
    "def eval_epoch(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, loss_fn, optimizer)\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    val_loss, val_accuracy = eval_epoch(model, val_dataloader, loss_fn)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_loss, test_accuracy = eval_epoch(model, test_dataloader, loss_fn)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Clear GPU memory after each epoch\n",
    "    #torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
